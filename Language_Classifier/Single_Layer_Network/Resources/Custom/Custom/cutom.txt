One of the main assumptions made when training learning systems is to suppose that the distribution of the inputs stays the same throughout the training. For linear models, which simply map input data to some appropriate outputs, this condition is always satisfied but it is not the case when dealing with Neural Networks which are composed of several layers stacked on top of each other.
In such architecture, each layer’s inputs are affected by the parameters of all preceding layers (small changes to the network parameters amplify as the network becomes deeper). As a consequence, a small change made during the backpropagation step within a layer can produce a huge variation of the inputs of another layer and at the end change feature maps distribution. During the training, each layer needs to continuously adapt to the new distribution obtained from the previous one and this slows down the convergence.
Batch normalization [1] overcomes this issue and make the training more efficient at the same time by reducing the covariance shift within internal layers (change in the distribution of network activations due to the change in network parameters during training) during training and with the advantages of working with batches.
This article will cover the following
How batch normalization can reduce internal covariance shift and how this can improve the training of a Neural Network.
How to implement a batch normalization layer in PyTorch.
Some simple experiments showing the advantages of using batch normalization.
Reduce internal covariance shift via mini-batch statistics
One way to reduce remove the ill effects of the internal covariance shift within a Neural Network is to normalize layers inputs. This operation not only enforces inputs to have the same distribution but also whitens each of them. This method is motivated by some studies [2] showing that the network training converges faster if its inputs are whitened and as a consequence, enforcing the whitening of the inputs of each layer is a desirable property for the network.
However, the full whitening of each layer’s inputs is costly and not fully differentiable. Batch normalization overcomes this issue by considering two assumptions:
instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently (by setting the mean of zero and the variance of 1).
instead of using the entire dataset to normalize activations, we use mini-batches as each mini-batch produces estimates of the mean and variance of each activation.
For a layer with d-dimensional inputs x = (x1, x2, .. xd) we obtain the normalization with the following formula (with expectation and variance computed over a batch B):

However, simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. Such a behavior is not desirable for the network as it will reduce his representative power (it would become equivalent to a single layer network).
